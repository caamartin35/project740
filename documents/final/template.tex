\documentclass[pageno]{jpaper}

%replace XXX with the submission number you are given from the HPCA submission site.
\newcommand{\hpcasubmissionnumber}{XXX}

\usepackage[normalem]{ulem}

\begin{document}

\title{Enhanced Base-Delta Compression with Data Splitting and Memory Pooling}
\author{Aditya Bhandaru (\href{mailto:akbhanda@andrew.cmu.edu}{akbhanda@andrew.cmu.edu}) Gennady Pekhimenko (\href{mailto:gpekhime@cs.cmu.edu}{akbhanda@andrew.cmu.edu}) \\
Onur Mutlu (\href{mailto:onur@cmu.edu}{akbhanda@andrew.cmu.edu})}
\date{}
\maketitle

\thispagestyle{empty}

\begin{abstract}
Recent literature on cache compression has shown great potential for increasing the effective cache capacity on chip. Specifically, a technique called Base-Delta (B+$\Delta$) compression has presented excellent compression (about 1.4X) and improvements in overall performance. However, B+$\Delta$ suffers from poor compressibility when adjacent data in memory have a high range in value.

We show here, as proof of concept, that existing techniques such as \textbf{Data Splitting and Memory Pooling can enhance B+$\Delta$ compressibility} for data in memory. Our simulations over various micro-benchmarks show that B+$\Delta$ with pooling  results in an 8\% reduction in MPKI, and a compression ratio of 2.6X over the baseline.
\end{abstract}

\section{Introduction}

\subsection{Background}

The memory bottleneck is a well known problem in computer architecture. Caching has become a standard for alleviating contention for data, the bus, and memory. As we trend to more cores, more applications, and larger computing problems, there is a much greater demand for data. Simply scaling cache size to compensate is too expensive, both in power and chip area.

Data compression in the cache is a promising alternative to increasing effective on chip cache capacity. For the same physical cache space, we can store more blocks per set. In general, compression algorithms look for patterns in data to exploit. Therefore, they are very sensitive to how data is laid out in memory and the kind of data a program manipulates.

The ideal cache compression implementation would be fast, simple, and offer a high compression. These design points are largely at odds with one another. For example, many ideas from older literature on cache compression suffer from either poor compression or incur high hardware complexity or long decompression latencies.

Why is fast decompression more important than fast compression? Decompression is on the critical path for a read. In order to supply the requested word, we must decompress the cache line. During a cache fill, compression can occur in the background while we bypass the requested word.

\subsection{Motivation}

Recently, a paper on Base-Delta-Immediate compression \cite{baseDeltaImm} suggested technique called Base+Delta (B+$\Delta$) compression. Base-Delta-Immediate (B$\Delta$I) compression is the final iteration of the technique. In comparison to preceding work on cache compression, it hits the fast-simple-high-compression trifecta nicely.

Their work yielded largely positive results, making B+$\Delta$ compression worthy of further study and improvement. In particular, we observe that improving data layout in memory to leverage B+$\Delta$ compression in hardware may result in substantial performance gains. This paper focuses on two existing techniques: data splitting and memory. As a proof of concept, we show that applying these transformations to programs running on B+$\Delta$ architectures will realize considerable gains.


\section{Previous Work}

\subsection{Cache-Aware Data Placement}

There is a substantial body of work on optimizing the arrangement of data in memory to improve temporal and spatial locality. Notably these approaches often involve compiler hints or runtime directives to a runtime library \cite{structLayoutHints} or allocator \cite{structLayoutAllocHints} for cache conscious data placement.

Further studies on dynamic analysis using profiling \cite{autoPoolAlloc} and even runtime structure splitting \cite{runtimeSplit} were introduced to reduce programmer effort. The latter technique offers great adaptability to the dynamic needs of the program, but incurs some overhead due to safety checks.

The idea of cache conscious data placement is not new. \textit{However, none of these previous works target systems that implement B+$\Delta$ cache compression}.


\subsection{Base-Delta Compression}

B+$\Delta$ compression \cite{baseDeltaImm} leverages the observation that for many cache lines, the values contained have a low dynamic range. That is, they could be encoded using a common base-value and a number of much smaller delta-values.

B+$\Delta$ compression does have its shortcomings. Every access and fill incurs and additional decompression and compression latency penalty respectively. Furthermore, certain benchmark programs manipulate data with very high dynamic range, particularly pointer based algorithms. B+$\Delta$ compression performs poorly here, and does not justify its inherent latency overhead.

For architectures that implement B+$\Delta$ cache compression, a cache conscious data placement policy would then place values with low dynamic range together, therefore minimizing the deltas. We anticipate that previously, poorly performing benchmarks would greatly benefit from such a data-layout transformation.


\subsection{Data Splitting and Memory Pooling}
We investigate two such mechanisms that place similar values together in memory: \textbf{data splitting and memory pooling}. As a proof of concept, this paper does not address the non-trivial implementation challenges that face the splitting and pooling of data. There is however, compelling work in the area such as MPADS \cite{mpads} and Forma \cite{forma}.

The basic idea is to analyze an object (perhaps a struct), and determine whether you can \textit{safely} allocate its members separately. This is the splitting phase. Next, we determine where we want to allocate the members. This is the pooling phase.

In the past, these techniques were intended to improve spatial locality. However, we can apply the same principles to help improve \textit{value}-locality. That is, to pool data of similar value together, exposing regions of low dynamic range for B+$\Delta$ compression to exploit.

Determining whether or not values are \textit{similar} can be done in a number of ways. Static type analysis, profiling, and runtime analysis are all possibilities.  Although, this is not a trivial problem.


\subsection{Why B+$\Delta$ and not B$\Delta$I?}

This paper focuses on B+$\Delta$ compression over B$\Delta$I compression for a couple reasons. First, we suspect that applying techniques such as data splitting and memory pooling will alleviate many of the low compressibility cases that B+$\Delta$ suffered on the benchmark tests. The second is simplicity. Encoding cache lines with only one base requires less metadata in the tag store and simpler hardware.


\section{B+$\Delta$ with Splitting and Pooling}

Our idea is simple. Arrange data in memory optimally for BD compression. Data splitting and memory pooling techniques can achieve this by placing data with similar value-behavior adjacently in memory.

\subsection{The Compression Algorithm}The key premise behind B+$\Delta$  compression is that cache lines with low dynamic range are abundant during runtime. That is, the mathematical differences between the word values are small compared to the size of the word.When this is the case, a cache line can be represented as a single \textit{base} value and an array of much smaller \textit{delta}-values (often half the size). As a result, we can encode the data in fewer total bytes. It follows that cache lines with high dynamic range cannot be compressed with B+$\Delta$ encoding.Benchmarks using pointer manipulation algorithms with large objects often suffer from poor compressibility. For instance, a linked-list traversal program may have many node structs allocated next to one another in memory. If each node consists of a flag, a counter, and a pointer, we might see boolean, integer, and pointer values interleaved in memory. Generally, a cache line with these types of interleaved values is incompressible.

\subsection{Low-Compressibility Cases}Let us start with a favorable case for B+$\Delta$ compression. For example, placing pointer-like values with other pointer-like values is preferable to having a pointer next to a boolean flag. The mathematic difference between a pointer-value and a boolean-value is obviously some large delta, which is useless for compression.  Less obvious, perhaps, is that the most significant bits of pointers are usually the same. If the first 32 bits are the same, we can use 4-byte deltas to encode a string of pointers relative to some 8-byte base (assuming a 64-bit architecture).However, benchmarks with heavy pointer manipulation and large objects often suffer from poor compressibility. For instance, a linked-list traversal program may have many node structs allocated next to one another in memory. If each node consists of a flag, a counter, and a next-pointer, we might see boolean, integer, and pointer values alternating in memory. Generally, cache lines with such interleaved values are incompressible.

\subsection{Data Splitting and Pooling} For such programs, we want to allocate objects such that we maintain spatial locality of similar-valued members. More elaborately, \textbf{split} an object up into its respective members. Allocate space for those members based on what kinds of values they hold--these decisions may be during compile time or runtime, depending on the pooling implementation. Members with similar value-types should be \textbf{pooled} (allocated) together.Expanding on the example from the previous section, we might have separate memory pools for flags, counters, and pointers. This creates contiguous regions of memory with much lower dynamic range. Now, when the next-pointer member of a node is read, the likelihood that the resulting cache fill is B+$\Delta$-compressible is far greater.The key observation here is that B+$\Delta$ compression  can strongly leverage any data transformations that create regions of low dynamic range.

\begin{figure}[bp]
  \centering
    \includegraphics[scale=0.27]{mechanisms.pdf}
\end{figure}

\subsection{Caveats}Some simplifications were made to the above examples for that sake of brevity. For example, a long string of boolean flags in memory is seldom B+$\Delta$-compressible. This is because B+$\Delta$ compression operates at 4-byte and 8-byte granularities. A string of 32 or 64 booleans has high entropy, even for biased distributions for true and false values. Interpreting these strings as integers and computing the differences may result in very large deltas, and low compressibility.

\section{Evaluation Methodology}This paper intends to show proof of concept. As such, we opted to evaluate over more specific metrics than straight IPC performance. Instead, we were more interested in compression ratio and a subsequent reduction in MPKI. We expect these metrics to strongly correlate with the computed IPC on a cycle accurate simulator.
\subsection{Benchmark Selection}Only three benchmarks were used in this study. This is largely because we have yet to implement the pointer transformations necessary for splitting and pooling in the LLVM compiler. Thus programs had to be converted manually, were smaller in size, and have smaller working sets. We refer to these programs as micro-benchmarks.Because of our limited benchmark suite, the programs were chosen carefully. We were particularly interested in how splitting and pooling might expose \textit{value}-locality in memory, even in pointer heavy algorithms with objects. A brief description of our micro-benchmarks follows:\begin{itemize}\item{\textbf{LLU:} Linked list traversal. Lists are grown each iteration, and counters within each node are incremented. This is an approximation for the better known\textit{Health} micro-benchmark.}\item{\textbf{Bisort:} This performs a bitonic sort on a randomly generated tree.}\item{\textbf{ArraySort:} A simple array sort. Elements are swapped and copied by value, not by pointer.}\end{itemize}


\subsection{Manual Splitting and Pooling}Splitting objects and pooling their members appropriately was done manually. Later we intend to perform these transformations in the compiler. Nevertheless, there are several drawbacks to this approach.Firstly, hand optimized code-quality is not necessarily achievable by a compiler. As a programmer, one has perfect information about safety, problem size, and the type of data intended for each struct member. Second, pooling often incurs a static memory overhead\cite{mpads}  because the last pool is not always filled (but memory is still reserved).We acknowledge that these factors concede some fidelity from the results. However, because this study is a proof of concept, we felt this was appropriate compromise.


\subsection{Cache Simulators}
Only a single level memory hierarchy is simulated here. Even though B+$\Delta$ compression is intended to run below the L1 cache, we do not omit L1 hits from our data. Normally the L2 would never see these requests. However, the validity of the data holds as both the baseline and B+$\Delta$ caches appreciate a proportional increase in hits.

\textbf{Baseline simulator} -- a 16-way, 32-byte block cache. The cache size is varied throughout the study to observe the interaction between capacity, the working set of the program, and the hit rate. The baseline cache implements a perfect least recently used (LRU) policy.

\textbf{B+$\Delta$ cache simulator} -- a 16-way, 32-byte block cache. Again, the cache capacity is varied throughout the study to observe the effect of small space constraints, large working sets, and pooling.The simulator implements a perfect LRU policy. When a newly inserted cache line exceeds the size of the set, LRU blocks are evicted until the new block fits. Recent literature, such as Cache Aware Replacement Policies\cite{carp} (CARP) suggests many optimizations to this policy, but they are not simulated here.


\bstctlcite{bstctl:etal, bstctl:nodash, bstctl:simpurl}
\bibliographystyle{IEEEtranS}
\bibliography{references}

\end{document}

